{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/code_sim/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from collections import deque\n",
    "from transformers import AutoTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_script(code):\n",
    "    \n",
    "    code_1 = code['code1']\n",
    "    code_2 = code['code2']\n",
    "\n",
    "    code_1_new = deque()\n",
    "    \n",
    "    anotation_checksum = 0\n",
    "    for line in code_1.split('\\n'):\n",
    "        if (line.lstrip().startswith(\"*/\") or line.rstrip().endswith(\"*/\")):\n",
    "            anotation_checksum = 0\n",
    "            continue\n",
    "        if anotation_checksum == 1:\n",
    "            continue\n",
    "        if line.lstrip().startswith('#include'): # #include으로 시작되는 행 skip\n",
    "            continue\n",
    "        if line.lstrip().startswith('/*'): # 주석시작\n",
    "            if \"*/\" in line:\n",
    "                continue\n",
    "            else:\n",
    "                anotation_checksum = 1 \n",
    "                continue\n",
    "\n",
    "        if line.lstrip().startswith('//'): # 주석으로 시작되는 행 skip\n",
    "            continue\n",
    "        line = line.rstrip()\n",
    "        if '//' in line:\n",
    "            line = line[:line.index('//')] # 주석 전까지 코드만 저장\n",
    "        line = line.replace('\\n','') # 개행 문자를 모두 삭제함\n",
    "        line = line.replace('    ','\\t') # 공백 4칸을 tab으로 변환\n",
    "        \n",
    "        if line.lstrip().rstrip() == '': # 전처리 후 빈 라인은 skipa\n",
    "            continue\n",
    "        \n",
    "        code_1_new.append(line)\n",
    "        \n",
    "    code_1_new = '\\n'.join(code_1_new) # 개행 문자로 합침\n",
    "    code_1_new = re.sub('(\"\"\"[\\w\\W]*?\"\"\")', '<str>', code_1_new)\n",
    "    code_1_new = re.sub(\"('''[\\w\\W]*?''')\", '<str>', code_1_new)\n",
    "    code_1_new = re.sub('/^(http?|https?):\\/\\/([a-z0-9-]+\\.)+[a-z0-9]{2,4}.*$/', '', code_1_new)\n",
    "    code['code1'] = code_1_new\n",
    "\n",
    "    code_2_new = deque()\n",
    "    \n",
    "    anotation_checksum = 0\n",
    "    for line in code_2.split('\\n'):\n",
    "        if (line.lstrip().startswith(\"*/\") or line.rstrip().endswith(\"*/\")):\n",
    "            anotation_checksum = 0\n",
    "            continue\n",
    "        if anotation_checksum == 1:\n",
    "            continue\n",
    "        if line.lstrip().startswith('#include'): # #include으로 시작되는 행 skip\n",
    "            continue\n",
    "        if line.lstrip().startswith('/*'): # 주석시작\n",
    "            if \"*/\" in line:\n",
    "                continue\n",
    "            else:\n",
    "                anotation_checksum = 1 \n",
    "                continue\n",
    "\n",
    "        if line.lstrip().startswith('//'): # 주석으로 시작되는 행 skip\n",
    "            continue\n",
    "        line = line.rstrip()\n",
    "        if '//' in line:\n",
    "            line = line[:line.index('//')] # 주석 전까지 코드만 저장\n",
    "        line = line.replace('\\n','') # 개행 문자를 모두 삭제함\n",
    "        line = line.replace('    ','\\t') # 공백 4칸을 tab으로 변환\n",
    "        \n",
    "        if line.lstrip().rstrip() == '': # 전처리 후 빈 라인은 skipa\n",
    "            continue\n",
    "        \n",
    "        code_2_new.append(line)\n",
    "        \n",
    "    code_2_new = '\\n'.join(code_2_new) # 개행 문자로 합침\n",
    "    code_2_new = re.sub('(\"\"\"[\\w\\W]*?\"\"\")', '<str>', code_2_new)\n",
    "    code_2_new = re.sub(\"('''[\\w\\W]*?''')\", '<str>', code_2_new)\n",
    "    code_2_new = re.sub('/^(http?|https?):\\/\\/([a-z0-9-]+\\.)+[a-z0-9]{2,4}.*$/', '', code_2_new)\n",
    "    code['code2'] = code_2_new\n",
    "\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['code1', 'code2', 'similar'], dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DDD = pd.read_csv('/home/workspace/DACON/CodeSim/Dataset/valid_data_lv1.csv')\n",
    "DDD.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (DataLoader ,RandomSampler, SequentialSampler, TensorDataset)\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "import transformers\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from transformers import AdamW, RobertaConfig\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             precision_recall_curve,\n",
    "                             f1_score,\n",
    "                             auc)\n",
    "import torch.nn as nn\n",
    "from transformers import (AutoConfig, \n",
    "                          AutoTokenizer, \n",
    "                          RobertaForSequenceClassification,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorWithPadding,\n",
    "                          EarlyStoppingCallback)\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import confusion_matrix, classification_report, matthews_corrcoef, f1_score, recall_score, precision_score\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "''' Tokenizer '''\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/graphcodebert-base')\n",
    "tokenizer.truncation_side = 'left' # 설정된 길이만큼 tokenize를 한 다음 초과되는 부분을 왼쪽에서 부터 자른다. \n",
    "\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding='max_length', max_length=512, truncation=True)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 595000/595000 [24:41<00:00, 401.72 examples/s] \n"
     ]
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_error()\n",
    "testdataset = load_dataset(\"csv\", data_files='/home/workspace/DACON/CodeSim/Dataset/test.csv')['train']\n",
    "\n",
    "preprocessed = testdataset.map(preprocess_script)\n",
    "test_dataset = preprocessed.map(example_fn, remove_columns=['code1', 'code2','pair_id'])\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "testloader = DataLoader(test_dataset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=False,\n",
    "                         collate_fn = collator\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37188/37188 [1:15:13<00:00,  8.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6585,  2.9356],\n",
       "        [ 1.7737, -1.8628],\n",
       "        [-0.7754,  0.8949],\n",
       "        ...,\n",
       "        [-1.4672,  1.6166],\n",
       "        [ 1.1838, -1.3130],\n",
       "        [ 2.1381, -2.3336]], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "load_path = \"/home/workspace/DACON/CodeSim/model/models/graphcodebert_Bs16_OptAdamW_ScduLinear_Sm0.0/1-fold/best.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(load_path, map_location=torch.device('cpu')))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "progress_bar = tqdm(enumerate(testloader), total=len(testloader), leave=True, position=0)\n",
    "for i, data in progress_bar:\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.from_numpy(np.asarray(data['input_ids']))\n",
    "        input_ids.to(device)\n",
    "        attention_mask = torch.from_numpy(np.asarray(data['attention_mask']))\n",
    "        attention_mask.to(device)\n",
    "        logits = model(input_ids.to(device), attention_mask.to(device))\n",
    "        logits=logits.logits\n",
    "    if i==0:\n",
    "      one_fold_logits = logits\n",
    "    else:\n",
    "      one_fold_logits = torch.cat([one_fold_logits,logits],dim=0)\n",
    "\n",
    "one_fold_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_fold_logits_new = one_fold_logits.squeeze(0).detach().cpu().numpy()\n",
    "all_fold_predictions = np.argmax(one_fold_logits_new, axis=1)\n",
    "all_fold_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_fold_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/home/workspace/DACON/CodeSim/Dataset/sample_submission.csv\")\n",
    "sub['similar'] = all_fold_predictions\n",
    "sub.to_csv('/home/workspace/DACON/CodeSim/Dataset/submission_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
