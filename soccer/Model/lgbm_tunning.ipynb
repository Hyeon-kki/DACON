{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "from bayes_opt import BayesianOptimization\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import *\n",
    "from preprocessing_utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = load_train_valid()\n",
    "train_x, train_y, valid_x, valid_y = preprocessing(train, valid, is_test=False)\n",
    "\n",
    "lec = LabelEncoder()\n",
    "train_y = lec.fit_transform(train_y)\n",
    "valid_y = lec.transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_params = {\n",
    "    'max_depth':(6, 16), #트리 최대 깊이\n",
    "    'num_leaves':(24,64), #트리 하나에 최대 잎 개수\n",
    "    'min_child_samples': (10, 200), #하나의 잎에 최소 데이터 개수 (오버피팅 대응)\n",
    "    'min_child_weight': (1, 50), #하나의 잎에 최소 sum hessian\n",
    "    'subsample': (0.5, 1), #\n",
    "    'colsample_bytree': (0.5, 1), #피처의 50% 를 트레이닝 전에 선택\n",
    "    'max_bin': (10, 500), #bins 의 최대 개수\n",
    "    'reg_lambda': (0.001, 10), #L2 regularization\n",
    "    'reg_alpha': (0.01, 50) #L1 regularization \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_logloss_eval(max_depth, num_leaves, min_child_samples, min_child_weight, subsample, colsample_bytree, max_bin, reg_lambda, reg_alpha):\n",
    "  params = {\n",
    "      \"n_estimators\" : 500, \"learning_rate\": 0.02,\n",
    "      'max_depth': int(round(max_depth)),\n",
    "      'num_leaves': int(round(num_leaves)),\n",
    "      'min_child_samples': int(round(min_child_samples)),\n",
    "      'min_child_weight': int(round(min_child_weight)),\n",
    "      'subsample': max(min(subsample, 1), 0), \n",
    "      'colsample_bytree': max(min(colsample_bytree, 1), 0),\n",
    "      'max_bin':  max(int(round(max_bin)),10),\n",
    "      'reg_lambda': max(reg_lambda,0),\n",
    "      'reg_alpha': max(reg_alpha, 0)\n",
    "\n",
    "  }\n",
    "  lgb_model = lgb.LGBMClassifier(**params)\n",
    "  lgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'logloss', verbose= 100, \n",
    "                early_stopping_rounds= 100)\n",
    "  valid_proba = lgb_model.predict_proba(valid_x)\n",
    "  logLoss = log_loss(valid_y, valid_proba)\n",
    "\n",
    "  return logLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BayesianOptimization' object has no attribute 'minimize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lgbBO \u001b[38;5;241m=\u001b[39m BayesianOptimization(f\u001b[38;5;241m=\u001b[39m lgb_logloss_eval, pbounds\u001b[38;5;241m=\u001b[39mbayesian_params, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlgbBO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m(init_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BayesianOptimization' object has no attribute 'minimize'"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(f= lgb_logloss_eval, pbounds=bayesian_params, random_state = 42)\n",
    "lgbBO.minimize(init_points = 5, n_iter = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightGBM\n",
    "lgbm = lgb.LGBMClassifier()\n",
    "lgbm.fit(train_x, train_y, eval_metric='logloss')\n",
    "prediction_lgbm = lgbm.predict_proba(valid_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
